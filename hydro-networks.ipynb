{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is hydro-networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import heapq\n",
    "from shapely.geometry import Point, LineString, MultiLineString, shape\n",
    "from os.path import join\n",
    "from IPython.display import display, Markdown\n",
    "from datetime import datetime\n",
    "import rasterio\n",
    "import pyproj\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in all the necessary files\n",
    "And do some quick sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_rivers = 'riv_capetown.gpkg'\n",
    "out_rivers = 'riv_capetown_processed.gpkg'\n",
    "out_nodes = 'riv_capetown_nodes.gpkg'\n",
    "out_hydro = 'riv_capetown_hydro.gpkg'\n",
    "\n",
    "in_dem = 'DEM/s35e015_con/s35e015_con/hdr.adf'\n",
    "in_gscd = 'GSCD/QMEAN.tif'\n",
    "in_flow_acc = 'FlowAcc/af_acc_15s/af_acc_15s/hrd.adf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the two rasters\n",
    "dem = rasterio.open(in_dem)\n",
    "gscd = rasterio.open(in_gscd)\n",
    "flow_acc = rasterio.open(in_flow_acc)\n",
    "\n",
    "rivers = gpd.read_file(in_rivers)\n",
    "\n",
    "# project to Mercator so our distances are in metres\n",
    "rivers = rivers.to_crs(epsg=3395)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(gscd.read(1), cmap='pink')\n",
    "pyplot.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(dem.read(1), cmap='pink')\n",
    "pyplot.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rivers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rivers.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create network and nodes\n",
    "These will be the main data structures for holding the river network, and the nodes where rivers join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the network and nodes from the HydroSHEDS layer, ready to be stream ordered\n",
    "\n",
    "Structure for network:\n",
    "0   index\n",
    "1   fx\n",
    "2   fy\n",
    "3   lx\n",
    "4   ly\n",
    "5   node index first point\n",
    "6   node index last point\n",
    "7   stream order\n",
    "8   arc length\n",
    "\n",
    "Structure for nodes:\n",
    "0   index\n",
    "1   x\n",
    "2   y\n",
    "3..   arc indices...\n",
    "\"\"\"\n",
    "\n",
    "# import the start and end end points into the network array of arcs\n",
    "# and simultaneously import them into a separate node array (so two nodes per arc)\n",
    "# there will be duplicate nodes, but they duplicates are ignored later\n",
    "counter = 0\n",
    "length = len(rivers)\n",
    "network = np.empty([length, 9], dtype=np.int32)\n",
    "nodes = []\n",
    "\n",
    "for index, row in rivers.iterrows():\n",
    "    # the geometry variables (int so that they actual match when comparing)\n",
    "    fx = int(row.geometry[0].xy[0][0])\n",
    "    fy = int(row.geometry[0].xy[1][0])\n",
    "    lx = int(row.geometry[0].xy[0][-1])\n",
    "    ly = int(row.geometry[0].xy[1][-1])\n",
    "\n",
    "    # add arc_length as a determinant of how much water contributes downstream\n",
    "    # not sure if it's a good idea going forward...\n",
    "    arc_length = int(row.geometry.length)\n",
    "    \n",
    "    # store the index as a column for easier access\n",
    "    # the last column is for stream order\n",
    "    network[counter] = [counter, fx, fy, lx, ly, -99, -99, -99, arc_length]\n",
    "\n",
    "    # create nodes for the start point and end point separately\n",
    "    nodes.append([2*counter, fx, fy, {}])\n",
    "    nodes.append([2*counter+1, lx, ly, {}])\n",
    "\n",
    "    counter += 1\n",
    "    #print('imported {}'.format(counter))\n",
    "\n",
    "# the most time consuming part of the script\n",
    "# runs through every arc and every node and where there is a match, add\n",
    "#  a reference to the index into the network array\n",
    "# once both nodes have been found for an arc, break that for loop and\n",
    "#  start with the next arc\n",
    "for arc in network:\n",
    "    match_f, match_l = False, False\n",
    "    for node in nodes:\n",
    "        # if the fx and fy values match\n",
    "        if arc[1] == node[1] and arc[2] == node[2]:\n",
    "            # add an index\n",
    "            arc[5] = node[0]\n",
    "            match_f = True\n",
    "        # if the lx and ly values match\n",
    "        elif arc[3] == node[1] and arc[4] == node[2]:\n",
    "            # add an index\n",
    "            arc[6] = node[0]\n",
    "            match_l = True\n",
    "        if match_f and match_l > 1:\n",
    "            # reset and skip to the next arc\n",
    "            break\n",
    "    #print('indexed {}'.format(arc[0]))\n",
    "\n",
    "\n",
    "\n",
    "# for every node, add references to every arc that connects to it\n",
    "for arc in network:\n",
    "    # tell the arc's starting node that it exists\n",
    "    nodes[arc[5]].append(arc[0])\n",
    "    # and tell the arc's ending node that it exists\n",
    "    nodes[arc[6]].append(arc[0])\n",
    "    # print('nodes {}'.format(arc[0]))\n",
    "\n",
    "#return network, nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Then calculate Shreve stream order for the network\n",
    "This allows to easily keep track of what is upstream and downstream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strahler stream order is not used but it's some pretty code so I'm keeping it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strahler(arc_index, direction_node_id, network, nodes):\n",
    "    \"\"\"\n",
    "    This function is nearly verbatim from Gleyzer2004 algorithm\n",
    "    But excludes the code to create river segments\n",
    "\n",
    "    :param arc_index: the index of an arc in the networks array\n",
    "    :param direction_node_id: the index of that arc's upstream node\n",
    "    :param network: the network of arcs\n",
    "    :param nodes: contains all the nodes and their connections\n",
    "    \"\"\"\n",
    "    up_stream_orders = []\n",
    "    if len(nodes[direction_node_id]) == 1:\n",
    "        network[arc_index][7] = 1\n",
    "    else:\n",
    "        for arc in nodes[direction_node_id]:\n",
    "            if network[arc][0] != arc_index:\n",
    "                if network[arc][5] != direction_node_id:\n",
    "                    up_stream_orders.append(strahler(arc, network[arc][5], network, nodes))\n",
    "                else:\n",
    "                    up_stream_orders.append(strahler(arc, network[arc][6], network, nodes))\n",
    "        max_order = 0\n",
    "        max_order_count = 0\n",
    "        for order in up_stream_orders:\n",
    "            if order > max_order:\n",
    "                max_order = order\n",
    "                max_order_count = 1\n",
    "            elif order == max_order:\n",
    "                max_order_count += 1\n",
    "        if max_order_count > 1:\n",
    "            network[arc_index][7] = max_order + 1\n",
    "        else:\n",
    "            network[arc_index][7] = max_order\n",
    "    print('so {}'.format(arc_index))\n",
    "    return network[arc_index][7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead we use Shreve stream order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shreve(arc_index, direction_node_id, network, nodes):\n",
    "    \"\"\"\n",
    "    Caclulate Shreve stream order instead of Strahler\n",
    "    This ensures that a downstream river is always higher\n",
    "    \"\"\"\n",
    "    up_stream_orders = []\n",
    "    if len(nodes[direction_node_id]) == 5:\n",
    "        network[arc_index][7] = 1\n",
    "    else:\n",
    "        for index, arc in enumerate(nodes[direction_node_id]):\n",
    "            if index >= 4:\n",
    "                if network[arc][0] != arc_index:\n",
    "                    if network[arc][5] != direction_node_id:\n",
    "                        up_stream_orders.append(shreve(arc, network[arc][5], network, nodes))\n",
    "                    else:\n",
    "                        up_stream_orders.append(shreve(arc, network[arc][6], network, nodes))\n",
    "\n",
    "        max_orders = heapq.nlargest(2, up_stream_orders)\n",
    "        if len(max_orders) == 2:\n",
    "            order = 0 + max_orders[0] + max_orders[1]\n",
    "        else:\n",
    "            order = 0 + max(up_stream_orders)\n",
    "\n",
    "        network[arc_index][7] = order\n",
    "\n",
    "    # print('so {}'.format(arc_index))\n",
    "    return network[arc_index][7]\n",
    "\n",
    "for node in nodes:\n",
    "    if len(node) == 5:  # only one arc connected\n",
    "        if node[1] == network[node[4]][3] and node[2] == network[node[4]][4]:\n",
    "            sink = network[node[4]][0]\n",
    "            network[sink][7] = shreve(sink, network[sink][5], network, nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import elevation, gscd etc for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = pyproj.Proj(init='epsg:3395')  # Mercator\n",
    "for node in nodes:\n",
    "    node_proj = proj(*node[1:3], inverse=True)\n",
    "    \n",
    "    node_elevation = next(dem.sample([node_proj]))[0]\n",
    "    node_runoff = next(gscd.sample([node_proj]))[0]\n",
    "    node_flow_acc = next(flow_acc.sample([node_proj]))[0]\n",
    "    \n",
    "    node[3] = {'elevation': node_elevation,\n",
    "               'runoff': node_runoff,\n",
    "               'flow_acc': node_flow_acc}\n",
    "    \n",
    "    ## Add:\n",
    "    # land_type\n",
    "    # et_ref\n",
    "    # precip (times 12!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W. Create hydropower points\n",
    "And add in data from dem, gscd, land_type, et_ref, precip\n",
    "\n",
    "**Need to multiply GSCD with HydroSHEDS FlowAcc layer to get from mm/year to m3/sec**\n",
    "\n",
    "Start at the top of each river, get source GSCDxFlowAccxsquare size.\n",
    "NB: Make the contribution % dependent on slope (elevation change/distance)!\n",
    "\n",
    "Copy paste from HydroModeller:\n",
    "For each node, subtract the flow accumulation for all upstream nodes, so that we calculate it's 'self discharge' only for it's directly contributing area if anything goes negative, give its original flow_acc back.\n",
    "\n",
    "Then we assign discharges to the network and make it a property of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the point_interval in metres\n",
    "# This loops through each stream and adds points down it at the specified interval\n",
    "# And creates a dict with these new point geometries\n",
    "\n",
    "# At 20 deg South:\n",
    "# 1 degree = 110704 metres -> 1 minute = 1845 metres -> 1 second = 30.75 metres\n",
    "# River network is 15 second resolution = 461 metres\n",
    "# Therefore each up_cell size is\n",
    "cell_area = (110704/60/60*15)**2\n",
    "\n",
    "\n",
    "proj = pyproj.Proj(init='epsg:3395')  # Mercator\n",
    "point_interval = 1000  # how frequently to make points\n",
    "head_calc_distance = 2000  # how far upstream to look to calculate head\n",
    "\n",
    "hydro_points_dicts = []\n",
    "count = 0\n",
    "for index, row in rivers.iterrows():\n",
    "    geom = shape(row['geometry'])\n",
    "    length = geom.length\n",
    "    for i, distance in enumerate(range(0, int(length), point_interval)):\n",
    "        arcid = row['arcid']\n",
    "        up_cells = row['up_cells']\n",
    "        \n",
    "        point = Point(proj(*list(geom.interpolate(distance).coords)[0], inverse=True))\n",
    "        upstream_point = Point(proj(*list(geom.interpolate(distance - head_calc_distance).coords)[0], inverse=True))\n",
    "        \n",
    "        elevation = next(dem.sample(list(point.coords))).tolist()[0]\n",
    "        upstream_elevation = next(dem.sample(list(upstream_point.coords))).tolist()[0]\n",
    "        head = upstream_elevation - elevation\n",
    "        \n",
    "        runoff = next(gscd.sample(list(point.coords))).tolist()[0]\n",
    "        flowrate = runoff * up_cells * cell_area * (1/1000) * (1/(8760*3600))  # convert mm/year to m3/s\n",
    "        \n",
    "        ## Add:\n",
    "        # land_type\n",
    "        # et_ref\n",
    "        # precip (times 12!!)\n",
    "        \n",
    "        rho = 1000\n",
    "        g = 9.81\n",
    "        n = 0.5\n",
    "        power = rho*g*n*flowrate*head\n",
    "        \n",
    "        if head > 0 and flowrate > 0:\n",
    "            hydro_points_dicts.append({'ARCID': arcid, 'elevation': elevation, \n",
    "                             'head': head, 'flowrate': flowrate, 'power': power, 'geometry': point})\n",
    "            \n",
    "        count += 1\n",
    "        if count % 10000 == 0: print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. Convert the data back to GeoDataFrame and save as GeoPackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df = pd.DataFrame(columns=['idx', 'xs', 'ys', 'xe', 'ye', 'node_start', 'node_end',\n",
    "                                   'so', 'length'], data=network)\n",
    "rivers_out = rivers.merge(network_df, how='left', left_index=True, right_index=True)\n",
    "rivers_out = rivers_out.to_crs(epsg=4326)\n",
    "\n",
    "nodes_for_df = [node[0:3] for node in nodes] # drop the extra columsn that will confuse a df\n",
    "for node in nodes:\n",
    "    nodes_for_df[node[0]].extend(list(node[3].values()))\n",
    "nodes_df = pd.DataFrame(columns=['idx', 'x', 'y', 'elevation', 'runoff', 'flow_acc'], data=nodes_for_df)\n",
    "nodes_geometry = [Point(xy) for xy in zip(nodes_df['x'], nodes_df['y'])]\n",
    "nodes_out = gpd.GeoDataFrame(nodes_df, crs=rivers.crs, geometry=nodes_geometry)\n",
    "nodes_out = nodes_out.to_crs(epsg=4326)\n",
    "\n",
    "# Convert the hydro_points to a GDF\n",
    "hydro_points = gpd.GeoDataFrame(hydro_points_dicts)\n",
    "hydro_points.crs = {'init' :'epsg:4326'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_out.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_points.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rivers_out.to_file(out_rivers, driver='GPKG')\n",
    "nodes_out.to_file(out_nodes, driver='GPKG')\n",
    "hydro_points.to_file(out_hydro, driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AS YET UNUSED FUNCTIONS BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_flow_acc(self):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # For each node, subtract the flow accumulation for all upstream nodes,\n",
    "    # so that we calculate it's 'self discharge' only for it's directly contributing area\n",
    "    # if anything goes negative, give its original flow_acc back\n",
    "    self.nodes_df[field_flow_acc_local] = 0\n",
    "    for index, node in self.nodes_df.iterrows():\n",
    "        actual_flowacc = self.nodes_df[field_flow_acc][index]\n",
    "        for arc in node[field_arcs]:\n",
    "            if self.network[arc][6] == index:  # the up arc flows *into* the node\n",
    "                subtract = self.nodes_df[field_flow_acc][self.network[arc][5]]\n",
    "                actual_flowacc -= subtract  # subtract upstream flowAcc\n",
    "        self.nodes_df.loc[index, field_flow_acc_local] = actual_flowacc\n",
    "    self.nodes_df.loc[self.nodes_df[field_flow_acc_local] < 0, field_flow_acc_local] = self.nodes_df[field_flow_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rainfall_runoff(self, default_precip_effectiveness, default_runoff_to_gw_fraction, runoff_calibration_accuracy):\n",
    "    \"\"\"\n",
    "    Calculate the runoff from the rainfall using the rainfall-runoff method\n",
    "    Calibrate against gscd annual values\n",
    "    \"\"\"\n",
    "    # precipitation_se = pd.read_excel('precip.xlsx', index_col=0, squeeze=True)\n",
    "    # self.precipitation_df = pd.DataFrame(columns=precipitation_se.index, index=self.nodes_df.index)\n",
    "    # for index, row in self.precipitation_df.iterrows():\n",
    "    #    self.precipitation_df.loc[index] = precipitation_se\n",
    "\n",
    "    self.runoff_df = pd.DataFrame(columns=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], index=self.nodes_df.index)\n",
    "\n",
    "    k_c = pd.read_excel('k_c.xlsx', index_col=0)\n",
    "\n",
    "    # formula from http://www.weap21.org/webhelp/hydrology.htm\n",
    "    # for each point, calculate the runoff for each month, then compare to gscd annual value and modify params\n",
    "    self.nodes_df['precip_effective'] = default_precip_effectiveness  # default starting values\n",
    "    self.nodes_df['runoff_to_gw_fraction'] = default_runoff_to_gw_fraction\n",
    "    for index, row in self.runoff_df.iterrows():\n",
    "        print('calibrate {}'.format(index))\n",
    "        counter = 0\n",
    "        while True:\n",
    "            for col in self.runoff_df:\n",
    "                et_ref = self.nodes_df.loc[index, field_et_ref]\n",
    "                # precip = self.precipitation_df.loc[index, col]\n",
    "                precip = self.nodes_df.loc[index, '{}{}'.format(field_precip, col)] * \\\n",
    "                         3600 * 24 * days_per_month[col]\n",
    "                precip_avail_for_et = precip * self.nodes_df.loc[index, 'precip_effective']\n",
    "                et_potential = et_ref * k_c.loc[col, self.nodes_df.loc[index, field_land_type]]\n",
    "                runoff = max(0, precip_avail_for_et-et_potential) + \\\n",
    "                    precip * (1-self.nodes_df.loc[index, 'precip_effective'])\n",
    "                runoff_to_surface = runoff * (1 - self.nodes_df.loc[index, 'runoff_to_gw_fraction'])\n",
    "\n",
    "                self.runoff_df.loc[index, col] = runoff_to_surface\n",
    "\n",
    "            ref_value = self.nodes_df.loc[index, field_gscd]\n",
    "            calc_value = self.runoff_df.loc[index].mean() * 12  # to compare with annual gscd\n",
    "\n",
    "            if counter > 10:\n",
    "                break\n",
    "            elif abs((calc_value - ref_value) / ref_value) < runoff_calibration_accuracy:\n",
    "                break\n",
    "            else:\n",
    "                counter += 1\n",
    "                self.nodes_df.loc[index, 'precip_effective'] *= 1 + sorted([-0.5, (calc_value - ref_value) / ref_value / 10, 0.5])[1]\n",
    "                self.nodes_df.loc[index, 'runoff_to_gw_fraction'] *= 1 + sorted([-0.5, (calc_value - ref_value) / ref_value / 10, 0.5])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_discharge(self, water_loss_factor, max_water_loss_fraction):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    area = float(arcpy.GetRasterProperties_management(self.fc_flow_acc, 'CELLSIZEX').getOutput(0)) ** 2\n",
    "\n",
    "    # calculate self discharge for each node\n",
    "    # Q [m3/s] = runoff [mm] * flowacc [number] * area [m2] / 8760 [h/year] * 3600 [s/h] * 1000 [mm/m]\n",
    "    self.discharge_df = pd.DataFrame(columns=self.runoff_df.columns, index=self.runoff_df.index)\n",
    "    for index, row in self.discharge_df.iterrows():\n",
    "        print('discharge {}'.format(index))\n",
    "        for col in self.discharge_df:\n",
    "            self.discharge_df.loc[index, col] = \\\n",
    "                self.runoff_df.loc[index, col]*self.nodes_df.loc[index, field_flow_acc_local] * \\\n",
    "                area / (24*days_per_month[col]*3600*1000)\n",
    "\n",
    "    self.nodes_df[field_discharge_local] = \\\n",
    "        self.nodes_df[field_gscd] * self.nodes_df[field_flow_acc_local] * area / (8760*3600*1000)\n",
    "    self.nodes_df[field_discharge_accumulated] = self.nodes_df[field_discharge_local]\n",
    "\n",
    "    # start from Shreve order 1, and contribute all discharges from upstream nodes to downstream nodes\n",
    "    for so in range(1, max(self.network.T[7])+1):\n",
    "        for arc in self.network:\n",
    "            if arc[7] == so:\n",
    "                print('contribute {}'.format(arc[0]))\n",
    "                self.nodes_df.loc[arc[6], field_discharge_accumulated] += \\\n",
    "                    self.nodes_df.loc[arc[5], field_discharge_accumulated] * \\\n",
    "                    (1 - max(max_water_loss_fraction, water_loss_factor * arc[8]))\n",
    "                for col in self.discharge_df:\n",
    "                    self.discharge_df.loc[arc[6], col] += \\\n",
    "                        self.discharge_df.loc[arc[5], col] * (1 - max(max_water_loss_fraction,\n",
    "                                                                      water_loss_factor * arc[8]))\n",
    "\n",
    "    self.nodes_df[field_discharge_max] = -99\n",
    "    self.nodes_df[field_discharge_mean] = -99\n",
    "    self.nodes_df[field_discharge_min] = -99\n",
    "    for index, node in self.nodes_df.iterrows():\n",
    "        self.nodes_df.loc[index, field_discharge_max] = self.discharge_df.loc[index].max()\n",
    "        self.nodes_df.loc[index, field_discharge_mean] = self.discharge_df.loc[index].mean()\n",
    "        self.nodes_df.loc[index, field_discharge_min] = self.discharge_df.loc[index].min()\n",
    "\n",
    "    # add the discharge from the upstream node of each arc to that arc\n",
    "    self.network_df = pd.DataFrame(self.network, columns=[field_arc_id, 'fx', 'fy', 'lx', 'ly',\n",
    "                                                          'nif', 'nil', field_shreve, 'arc_length'])\n",
    "    self.network_df[field_discharge_accumulated] = -99\n",
    "    self.network_df[field_discharge_max] = -99\n",
    "    self.network_df[field_discharge_mean] = -99\n",
    "    self.network_df[field_discharge_min] = -99\n",
    "    for index, arc in self.network_df.iterrows():\n",
    "        self.network_df.loc[index, field_discharge_accumulated] = \\\n",
    "            self.nodes_df[field_discharge_accumulated][arc['nif']]\n",
    "        self.network_df.loc[index, field_discharge_max] = self.nodes_df[field_discharge_max][arc['nif']]\n",
    "        self.network_df.loc[index, field_discharge_mean] = self.nodes_df[field_discharge_mean][arc['nif']]\n",
    "        self.network_df.loc[index, field_discharge_min] = self.nodes_df[field_discharge_min][arc['nif']]\n",
    "\n",
    "    # add the results back into the ArcGIS feature\n",
    "    arcpy.AddField_management(self.fc_rivers, field_arc_id, 'LONG')\n",
    "    arcpy.AddField_management(self.fc_rivers, field_shreve, 'LONG')\n",
    "    arcpy.AddField_management(self.fc_rivers, field_discharge_accumulated, 'DOUBLE')\n",
    "    arcpy.AddField_management(self.fc_rivers, field_discharge_max, 'DOUBLE')\n",
    "    arcpy.AddField_management(self.fc_rivers, field_discharge_mean, 'DOUBLE')\n",
    "    arcpy.AddField_management(self.fc_rivers, field_discharge_min, 'DOUBLE')\n",
    "    with arcpy.da.UpdateCursor(self.fc_rivers, [field_arc_id, field_shreve, field_discharge_accumulated,\n",
    "                                                field_discharge_max, field_discharge_mean,\n",
    "                                                field_discharge_min]) as cursor:\n",
    "        for index, row in enumerate(cursor, start=0):\n",
    "            row[0] = self.network_df.loc[index, field_arc_id]\n",
    "            row[1] = self.network_df.loc[index, field_shreve]\n",
    "            row[2] = self.network_df.loc[index, field_discharge_accumulated]\n",
    "            row[3] = self.network_df.loc[index, field_discharge_max]\n",
    "            row[4] = self.network_df.loc[index, field_discharge_mean]\n",
    "            row[5] = self.network_df.loc[index, field_discharge_min]\n",
    "            cursor.updateRow(row)\n",
    "            print('added {}'.format(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hydro_potential(self, interval, eta_t, eta_g, conv):\n",
    "    \"\"\"\n",
    "    Insert points at intervals along every stream and estimate the hydro potential at each\n",
    "    \"\"\"\n",
    "\n",
    "    desc = arcpy.Describe(self.fc_rivers)\n",
    "    arcpy.CreateFeatureclass_management(arcpy.env.workspace, self.fc_points, geometry_type='POINT',\n",
    "                                        spatial_reference=desc.spatialReference)\n",
    "    arcpy.AddField_management(self.fc_points, field_arc_id, 'LONG')\n",
    "\n",
    "    with arcpy.da.SearchCursor(self.fc_rivers, ['SHAPE@', field_arc_id]) as search_cursor:\n",
    "        with arcpy.da.InsertCursor(self.fc_points, ['SHAPE@', field_arc_id]) as insert_cursor:\n",
    "            for row in search_cursor:\n",
    "                print('pointsifying {}'.format(row[1]))\n",
    "                line = row[0]\n",
    "\n",
    "                if line:\n",
    "                    cur_length = interval\n",
    "                    max_position = line.length\n",
    "                    insert_cursor.insertRow([line.firstPoint, row[1]])\n",
    "                    while cur_length < max_position:\n",
    "                        insert_cursor.insertRow([line.positionAlongLine(cur_length, False), row[1]])\n",
    "                        cur_length += interval\n",
    "\n",
    "    arcpy.CheckOutExtension('Spatial')\n",
    "    arcpy.sa.ExtractMultiValuesToPoints(self.fc_points, [[self.fc_elevation, field_elevation]])\n",
    "    arcpy.CheckInExtension('Spatial')\n",
    "\n",
    "    self.points_df = pd.DataFrame(arcpy.da.TableToNumPyArray(self.fc_points, (field_arc_id, field_elevation),\n",
    "                                                        skip_nulls=False, null_value=0))\n",
    "\n",
    "    # transfer discharge values from the river to the points\n",
    "    self.points_df[field_discharge_accumulated] = -99\n",
    "    self.points_df[field_discharge_max] = -99\n",
    "    self.points_df[field_discharge_mean] = -99\n",
    "    self.points_df[field_discharge_min] = -99\n",
    "    for index, row in self.points_df.iterrows():\n",
    "        self.points_df.loc[index, field_discharge_accumulated] = \\\n",
    "            self.network_df.loc[self.points_df.loc[index, field_arc_id], field_discharge_accumulated]\n",
    "        self.points_df.loc[index, field_discharge_max] = \\\n",
    "            self.network_df.loc[self.points_df.loc[index, field_arc_id], field_discharge_max]\n",
    "        self.points_df.loc[index, field_discharge_mean] = \\\n",
    "            self.network_df.loc[self.points_df.loc[index, field_arc_id], field_discharge_mean]\n",
    "        self.points_df.loc[index, field_discharge_min] = \\\n",
    "            self.network_df.loc[self.points_df.loc[index, field_arc_id], field_discharge_min]\n",
    "\n",
    "    # calculate the elevation difference (head) between successive points\n",
    "    self.points_df[field_head] = -99\n",
    "    prev_fid = 0\n",
    "    prev_elev = 0\n",
    "    for index, point in self.points_df.iterrows():\n",
    "        print('head {}'.format(index))\n",
    "        if point[field_elevation]:  # check to make sure an elevation entry exists\n",
    "            current_fid = point[field_arc_id]\n",
    "            current_elev = point[field_elevation]\n",
    "\n",
    "            if (current_fid == prev_fid) and ((prev_elev - current_elev) > 0):\n",
    "                self.points_df.loc[index, field_head] = prev_elev - current_elev\n",
    "            else:\n",
    "                self.points_df.loc[index, field_head] = 0\n",
    "\n",
    "            prev_fid = current_fid\n",
    "            prev_elev = current_elev\n",
    "        else:\n",
    "            self.points_df.loc[index, field_head] = 0\n",
    "            prev_fid = point[field_arc_id]\n",
    "            prev_elev = 0\n",
    "\n",
    "    # calculate the power in watts based on Alex's formula\n",
    "    # P = rho * g * eta_t * eta_g * conv * Q * deltaH\n",
    "    rho = 1000  # density\n",
    "    g = 9.80665  # gravity\n",
    "    self.points_df[field_power] = rho * g * eta_t * eta_g * conv * \\\n",
    "        self.points_df[field_discharge_accumulated] * self.points_df[field_head]\n",
    "    self.points_df[field_power_max] = rho * g * eta_t * eta_g * conv * \\\n",
    "        self.points_df[field_discharge_max] * self.points_df[field_head]\n",
    "    self.points_df[field_power_mean] = rho * g * eta_t * eta_g * conv * \\\n",
    "        self.points_df[field_discharge_mean] * self.points_df[field_head]\n",
    "    self.points_df[field_power_min] = rho * g * eta_t * eta_g * conv * \\\n",
    "        self.points_df[field_discharge_min] * self.points_df[field_head]\n",
    "\n",
    "    # add the results back into the ArcGIS feature\n",
    "    arcpy.AddField_management(self.fc_points, field_discharge_accumulated, 'DOUBLE')\n",
    "    arcpy.AddField_management(self.fc_points, field_discharge_max, 'DOUBLE')\n",
    "    arcpy.AddField_management(self.fc_points, field_discharge_mean, 'DOUBLE')\n",
    "    arcpy.AddField_management(self.fc_points, field_discharge_min, 'DOUBLE')\n",
    "    arcpy.AddField_management(self.fc_points, field_head, 'FLOAT')\n",
    "    arcpy.AddField_management(self.fc_points, field_power, 'DOUBLE')\n",
    "    arcpy.AddField_management(self.fc_points, field_power_max, 'DOUBLE')\n",
    "    arcpy.AddField_management(self.fc_points, field_power_mean, 'DOUBLE')\n",
    "    arcpy.AddField_management(self.fc_points, field_power_min, 'DOUBLE')\n",
    "    with arcpy.da.UpdateCursor(self.fc_points, [field_discharge_accumulated, field_discharge_max,\n",
    "                                                field_discharge_mean, field_discharge_min, field_head, field_power,\n",
    "                                                field_power_max, field_power_mean, field_power_min]) as cursor:\n",
    "        for index, row in enumerate(cursor, start=0):\n",
    "            row[0] = self.points_df.loc[index, field_discharge_accumulated]\n",
    "            row[1] = self.points_df.loc[index, field_discharge_max]\n",
    "            row[2] = self.points_df.loc[index, field_discharge_mean]\n",
    "            row[3] = self.points_df.loc[index, field_discharge_min]\n",
    "            row[4] = self.points_df.loc[index, field_head]\n",
    "            row[5] = self.points_df.loc[index, field_power]\n",
    "            row[6] = self.points_df.loc[index, field_power_max]\n",
    "            row[7] = self.points_df.loc[index, field_power_mean]\n",
    "            row[8] = self.points_df.loc[index, field_power_min]\n",
    "            cursor.updateRow(row)\n",
    "            print('added {}'.format(index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
